Testing overfitting the model on all the datasets
Class ratio [('AMV0', 10, 0.014285714285714285), ('Argaen0', 10, 0.014285714285714285), ('Bastiandantilus0', 10, 0.014285714285714285), ('Binnie0', 10, 0.014285714285714285), ('BlackEagle0', 10, 0.014285714285714285), ('ChevalierMalFet0', 10, 0.014285714285714285), ('Coconut Big0', 10, 0.014285714285714285), ('Eko0', 10, 0.014285714285714285), ('EnTerr0', 10, 0.014285714285714285), ('Entropy0', 10, 0.014285714285714285), ('Fizu0', 10, 0.014285714285714285), ('GauravRai0', 10, 0.014285714285714285), ('Greatlemer0', 10, 0.014285714285714285), ('IdahoJacket0', 10, 0.014285714285714285), ('IdoLivneh0', 10, 0.014285714285714285), ('J3ffreySmith0', 10, 0.014285714285714285), ('Michael0', 10, 0.014285714285714285), ('NaN0', 10, 0.014285714285714285), ('Nooodles0', 10, 0.014285714285714285), ('Phayr0', 10, 0.014285714285714285), ('RalfKistner0', 10, 0.014285714285714285), ('SickMath0', 10, 0.014285714285714285), ('ToR0', 10, 0.014285714285714285), ('YOBA0', 10, 0.014285714285714285), ('addie90000', 10, 0.014285714285714285), ('alexamici0', 10, 0.014285714285714285), ('ana valeije0', 10, 0.014285714285714285), ('anb0', 10, 0.014285714285714285), ('aosuka0', 10, 0.014285714285714285), ('bigOnion0', 10, 0.014285714285714285), ('caethan0', 10, 0.014285714285714285), ('cathco0', 10, 0.014285714285714285), ('cheilman0', 10, 0.014285714285714285), ('fractal0', 10, 0.014285714285714285), ('gepa0', 10, 0.014285714285714285), ('gizzywump0', 10, 0.014285714285714285), ('graygrass0', 10, 0.014285714285714285), ('hannanaha0', 10, 0.014285714285714285), ('imakaramegane0', 10, 0.014285714285714285), ('int n0', 10, 0.014285714285714285), ('j4b0', 10, 0.014285714285714285), ('jakab9220', 10, 0.014285714285714285), ('jgaten0', 10, 0.014285714285714285), ('joegunrok0', 10, 0.014285714285714285), ('kawasaki0', 10, 0.014285714285714285), ('kmod0', 10, 0.014285714285714285), ('lookingfor0', 10, 0.014285714285714285), ('max bublis0', 10, 0.014285714285714285), ('mth0', 10, 0.014285714285714285), ('netsuso0', 10, 0.014285714285714285), ('nlse0', 10, 0.014285714285714285), ('nwin0', 10, 0.014285714285714285), ('oonishi0', 10, 0.014285714285714285), ('pavlovic0', 10, 0.014285714285714285), ('pawko0', 10, 0.014285714285714285), ('pek0', 10, 0.014285714285714285), ('pyronimous0', 10, 0.014285714285714285), ('radkokotev0', 10, 0.014285714285714285), ('rainmayecho0', 10, 0.014285714285714285), ('raja baz0', 10, 0.014285714285714285), ('rmmh0', 10, 0.014285714285714285), ('ronnodas0', 10, 0.014285714285714285), ('royf0', 10, 0.014285714285714285), ('serialk0', 10, 0.014285714285714285), ('shishkander0', 10, 0.014285714285714285), ('taichino0', 10, 0.014285714285714285), ('tama eguchi0', 10, 0.014285714285714285), ('xoxie0', 10, 0.014285714285714285), ('yordanmiladinov0', 10, 0.014285714285714285), ('ziyan0', 10, 0.014285714285714285)]
Train labels :(560,80.0%)
Test  labels :(140,20.0%)
Model: 1_lstm_no_dropout_500 
		embed EmbedID ['W'] 
				 W => (108, 500)
		lstm1 
			lateral Linear ['W'] 
						 W => (2000, 500)
			upward Linear ['W', 'b'] 
						 W => (2000, 500)
						 b => (2000,)
		w Linear ['W', 'b'] 
				 W => (70, 500)
				 b => (70,)
Optimizer: ('MomentumSGD', {'momentum': 0.9, 'lr': 0.01}) 
Evaluation
epoch	training loss	test loss	test accuracy
1	3.814623546600342	3.1035774299076624	0.21428571428571427
2	2.7315789933715546	2.453481490271432	0.3357142857142857
3	2.0066022400345123	1.9957441415105548	0.4714285714285714
4	1.4766440979072026	1.5811026624270847	0.5642857142857143
5	1.0737135482685907	1.6415305903979711	0.5642857142857143
6	0.7190415173768997	1.5241439121110099	0.65
7	0.46172685538019453	1.3941984363964626	0.6428571428571429
8	0.3133446310247694	1.9156047804015024	0.5857142857142857
9	0.22704141991479057	1.3854054995945522	0.6928571428571428
10	0.11593963759286063	1.554112901006426	0.6857142857142857
11	0.06253337349210467	1.377960729598999	0.7071428571428572
12	0.023878041335514615	1.4382203136171614	0.7214285714285714
13	0.008375486305781773	1.4575532674789429	0.7214285714285714
14	0.005138637338365827	1.428919611658369	0.7214285714285714
15	0.0030127201761518205	1.4408102682658603	0.7214285714285714
16	0.0025242958750043595	1.4499132326671056	0.7285714285714285
17	0.002190784045628139	1.4570745025362288	0.7214285714285714
18	0.001939974512372698	1.4639849730900356	0.7285714285714285
19	0.0017415949276515415	1.4696451970509121	0.7285714285714285
20	0.0015800918851579938	1.4754699741091046	0.7285714285714285
21	0.0014491575104849678	1.4806548697607858	0.7285714285714285
22	0.0013381634439740863	1.4851022788456507	0.7285714285714285
23	0.0012430242129734584	1.4889544214521135	0.7285714285714285
24	0.0011596083641052246	1.492774554661342	0.7285714285714285
25	0.0010879499571663993	1.4963248423167637	0.7285714285714285
26	0.001023871558053153	1.49989264692579	0.7285714285714285
27	0.0009665983063834055	1.502679399081639	0.7285714285714285
28	0.0009160416466849191	1.506095814704895	0.7285714285714285
29	0.0008706654821123396	1.5089703457696098	0.7285714285714285
30	0.0008292181151253837	1.5120038475309099	0.7285714285714285
31	0.0007918357849121093	1.514789949144636	0.7285714285714285
32	0.0007571969713483538	1.5171848228999547	0.7285714285714285
33	0.0007257495607648577	1.519598937034607	0.7285714285714285
34	0.0006961652210780553	1.5218193599155971	0.7285714285714285
35	0.0006697569574628557	1.5241714715957642	0.7357142857142858
36	0.0006449154445103237	1.5260791574205672	0.7357142857142858
37	0.0006218671798706055	1.5280522244317192	0.7357142857142858
38	0.0006002255848475865	1.5302479914256504	0.7357142857142858
