Testing overfitting the model on all the datasets
Class ratio [('AMV0', 10, 0.014285714285714285), ('Argaen0', 10, 0.014285714285714285), ('Bastiandantilus0', 10, 0.014285714285714285), ('Binnie0', 10, 0.014285714285714285), ('BlackEagle0', 10, 0.014285714285714285), ('ChevalierMalFet0', 10, 0.014285714285714285), ('Coconut Big0', 10, 0.014285714285714285), ('Eko0', 10, 0.014285714285714285), ('EnTerr0', 10, 0.014285714285714285), ('Entropy0', 10, 0.014285714285714285), ('Fizu0', 10, 0.014285714285714285), ('GauravRai0', 10, 0.014285714285714285), ('Greatlemer0', 10, 0.014285714285714285), ('IdahoJacket0', 10, 0.014285714285714285), ('IdoLivneh0', 10, 0.014285714285714285), ('J3ffreySmith0', 10, 0.014285714285714285), ('Michael0', 10, 0.014285714285714285), ('NaN0', 10, 0.014285714285714285), ('Nooodles0', 10, 0.014285714285714285), ('Phayr0', 10, 0.014285714285714285), ('RalfKistner0', 10, 0.014285714285714285), ('SickMath0', 10, 0.014285714285714285), ('ToR0', 10, 0.014285714285714285), ('YOBA0', 10, 0.014285714285714285), ('addie90000', 10, 0.014285714285714285), ('alexamici0', 10, 0.014285714285714285), ('ana valeije0', 10, 0.014285714285714285), ('anb0', 10, 0.014285714285714285), ('aosuka0', 10, 0.014285714285714285), ('bigOnion0', 10, 0.014285714285714285), ('caethan0', 10, 0.014285714285714285), ('cathco0', 10, 0.014285714285714285), ('cheilman0', 10, 0.014285714285714285), ('fractal0', 10, 0.014285714285714285), ('gepa0', 10, 0.014285714285714285), ('gizzywump0', 10, 0.014285714285714285), ('graygrass0', 10, 0.014285714285714285), ('hannanaha0', 10, 0.014285714285714285), ('imakaramegane0', 10, 0.014285714285714285), ('int n0', 10, 0.014285714285714285), ('j4b0', 10, 0.014285714285714285), ('jakab9220', 10, 0.014285714285714285), ('jgaten0', 10, 0.014285714285714285), ('joegunrok0', 10, 0.014285714285714285), ('kawasaki0', 10, 0.014285714285714285), ('kmod0', 10, 0.014285714285714285), ('lookingfor0', 10, 0.014285714285714285), ('max bublis0', 10, 0.014285714285714285), ('mth0', 10, 0.014285714285714285), ('netsuso0', 10, 0.014285714285714285), ('nlse0', 10, 0.014285714285714285), ('nwin0', 10, 0.014285714285714285), ('oonishi0', 10, 0.014285714285714285), ('pavlovic0', 10, 0.014285714285714285), ('pawko0', 10, 0.014285714285714285), ('pek0', 10, 0.014285714285714285), ('pyronimous0', 10, 0.014285714285714285), ('radkokotev0', 10, 0.014285714285714285), ('rainmayecho0', 10, 0.014285714285714285), ('raja baz0', 10, 0.014285714285714285), ('rmmh0', 10, 0.014285714285714285), ('ronnodas0', 10, 0.014285714285714285), ('royf0', 10, 0.014285714285714285), ('serialk0', 10, 0.014285714285714285), ('shishkander0', 10, 0.014285714285714285), ('taichino0', 10, 0.014285714285714285), ('tama eguchi0', 10, 0.014285714285714285), ('xoxie0', 10, 0.014285714285714285), ('yordanmiladinov0', 10, 0.014285714285714285), ('ziyan0', 10, 0.014285714285714285)]
Train labels :(560,80.0%)
Test  labels :(140,20.0%)
Model: 1_lstm_no_dropout_500_labels 
		embed EmbedID ['W'] 
				 W => (108, 500)
		lstm1 
			upward Linear ['W', 'b'] 
						 W => (2000, 500)
						 b => (2000,)
			lateral Linear ['W'] 
						 W => (2000, 500)
		w Linear ['W', 'b'] 
				 W => (70, 500)
				 b => (70,)
Optimizer: ('MomentumSGD', {'momentum': 0.9, 'lr': 0.01}) 
Evaluation
epoch	training loss	test loss	test accuracy
1	3.800106564589909	3.0050008296966553	0.22857142857142856
2	2.649484659518514	2.3639051062720164	0.3142857142857143
3	1.9387419411114284	2.0711948088237215	0.4642857142857143
4	1.361685116801943	1.8666389908109393	0.4714285714285714
5	0.9345228714602334	1.6654838493892126	0.5357142857142857
6	0.6592885617698941	1.5757822666849408	0.6285714285714286
7	0.46002616201128277	1.5201027819088526	0.6571428571428571
8	0.326579270192555	1.5500212362834385	0.6714285714285714
9	0.14530344690595354	1.4896878004074097	0.6785714285714286
10	0.058347819532666885	1.5621303217751639	0.6785714285714286
11	0.04716861077717372	1.545586361203875	0.7
12	0.038730388028281076	1.415918963296073	0.6928571428571428
13	0.017125495842524936	1.4503150769642421	0.7142857142857143
14	0.0062967164175851	1.5378767388207573	0.6928571428571428
15	0.006758203676768711	1.4803491967064994	0.7
16	0.002660339219229562	1.4996283088411604	0.7
17	0.0023040618215288433	1.5127597672598703	0.7142857142857143
18	0.002073831217629569	1.5229926075254168	0.7071428571428572
19	0.0019028919083731515	1.5300861801419938	0.7071428571428572
20	0.001780198301587786	1.5358811821256364	0.7
21	0.0016864112445286342	1.540856855256217	0.6928571428571428
22	0.0016038060188293458	1.5447410583496093	0.6928571428571428
23	0.0015458754130772182	1.5464115858078002	0.6857142857142857
24	0.0014959096908569336	1.5484321594238282	0.6857142857142857
25	0.0014534814017159597	1.5507892676762172	0.6857142857142857
26	0.0014188970838274275	1.5516781977244787	0.6857142857142857
27	0.0013881002153669085	1.553949056352888	0.6857142857142857
28	0.0013646125793457032	1.5533663068498884	0.6928571428571428
29	0.0013443538120814732	1.5545249053410122	0.7
30	0.0013234359877450125	1.5550447021211897	0.7
31	0.0013141836438860211	1.5547105584825789	0.7
32	0.00129840544291905	1.5542136124202184	0.7
33	0.0012868881225585938	1.55493186882564	0.7
34	0.0012763755662100655	1.5555850676127843	0.7
35	0.001266305787222726	1.555074051448277	0.7
36	0.0012624604361397879	1.5566249813352313	0.7
37	0.0012559992926461357	1.555985563141959	0.7
38	0.0012515783309936523	1.5535987854003905	0.7
39	0.0012451274054391043	1.5546005044664655	0.7
40	0.0012405821255275181	1.557445342200143	0.6928571428571428
41	0.001239987782069615	1.5536390611103603	0.6928571428571428
42	0.0012345688683646067	1.5532586847032819	0.6928571428571428
43	0.0012319837297712054	1.5540065458842687	0.6928571428571428
44	0.0012334363801138743	1.554909586906433	0.6928571428571428
45	0.0012325235775538853	1.5543438366481237	0.6928571428571428
46	0.0012203523090907505	1.5531541688101633	0.6928571428571428
47	0.001226074354989188	1.5526496750967844	0.6928571428571428
48	0.0012280702590942382	1.5534675530024937	0.6928571428571428
49	0.0012228318623134068	1.554152022089277	0.7
50	0.0012236901691981723	1.5526739154543197	0.6928571428571428
